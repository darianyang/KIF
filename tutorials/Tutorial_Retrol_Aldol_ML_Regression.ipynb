{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial for Performing Supervised Learning (Regression) on Simulations of a Retro Aldolase Enzyme\n",
    "\n",
    "In this jupyter notebook we will use the model_building.py module to perform supervised machine learning (regression) on the retro-aldolase enzyme XXXX. \n",
    "\n",
    "Our target variable (often called \"y\") in this case is a continous measurement of the catalytic distance between the XXXX and XXXX (see Image below).\n",
    "In this case, a lower catalytic distance will give rise to an improved accuracy. \n",
    "\n",
    "This notebook will also cover all the pre- and post-processing steps requireds to prepare, analyse and visualise the results.\n",
    "\n",
    "The dataset used here is the same as what was used in the manuscript. \n",
    "\n",
    "<center><img src=\"miscellaneous/TODO.png\" style=\"width: 70%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # note temporary... \n",
    "sys.path.append(\"..\") # note temporary...\n",
    "\n",
    "from key_interactions_finder import pycontact_processing\n",
    "from key_interactions_finder import data_preperation\n",
    "from key_interactions_finder import model_building\n",
    "from key_interactions_finder import post_proccessing\n",
    "from key_interactions_finder import pymol_projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Process PyContact files with the pycontact_processing.py module \n",
    "\n",
    "In this section we will work with the PyContact output files generated. \n",
    "Here we will merge our seperate runs together and remove any false interactions that can be generated by the PyContact library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycontact_files_horizontal = [\"PyContact_Per_Frame_Interactions_Block1.csv\", \"PyContact_Per_Frame_Interactions_Block2.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block3.csv\", \"PyContact_Per_Frame_Interactions_Block4.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block5.csv\", \"PyContact_Per_Frame_Interactions_Block6.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block7.csv\", \"PyContact_Per_Frame_Interactions_Block8.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block9.csv\", \"PyContact_Per_Frame_Interactions_Block10.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block11.csv\", \"PyContact_Per_Frame_Interactions_Block12.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block13.csv\", \"PyContact_Per_Frame_Interactions_Block14.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block15.csv\", \"PyContact_Per_Frame_Interactions_Block16.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block17.csv\"]\n",
    "\n",
    "pycontact_dataset = pycontact_processing.PyContactInitializer(\n",
    "    pycontact_files=pycontact_files_horizontal,\n",
    "    multiple_files=True,\n",
    "    merge_files_method=\"horizontal\",  \n",
    "    remove_false_interactions=True,\n",
    "    in_dir=\"datasets/retrol_aldolase_data/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As outputted above, we can inspect the newly prepared dataset by accessing the '.prepared_df' class attribute as follows:\n",
    "pycontact_dataset.prepared_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Prepare the Dataset for Machine Learning with the data_preperation.py module. \n",
    "\n",
    "In this step, we take our processed dataframe and merge our per frame measurement of catalytic distances to it.\n",
    "We can also optionally perform several forms of filtering to select what types of interactions we\n",
    "would like to study.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we generate an instance of the SupervisedFeatureData class (because we have per frame class labels).\n",
    "catalytic_distances_file = \"datasets/retrol_aldolase_data/Cat_Distance_4a2s_RA95_5.txt\"\n",
    "\n",
    "supervised_dataset = data_preperation.SupervisedFeatureData(\n",
    "    input_df=pycontact_dataset.prepared_df,\n",
    "    target_file=catalytic_distances_file,\n",
    "    is_classification=False,\n",
    "    header_present=True # If your target_file has a header present, set to True.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated above to access the newly generated dataframe we can use the class attribute as follows\n",
    "supervised_dataset.df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional Feature Filtering\n",
    "\n",
    "In the above dataframe we have 3057 columns (so 3056 features + 1 target). We can take all of these forward for the stastical analysis or we can perform some filtering in advance (the choice is yours). \n",
    "There are five built in filtering methods available to you to perform filtering:\n",
    "\n",
    "1. **filter_by_occupancy(min_occupancy)** - Remove features that have an %occupancy less than the provided cut-off. %Occupancy is the % of frames with a non 0 value, i.e. the interaction is present in that frame.\n",
    "\n",
    "2. **filter_by_interaction_type(interaction_types_included)** - PyContact defines four types of interactions (\"Hbond\", \"Saltbr\", \"Hydrophobic\", \"Other\"). You select the interactions your want to include.\n",
    "\n",
    "3. **filter_by_main_or_side_chain(main_side_chain_types_included)** - PyContact can also define if each interaction is primarily from the backbone or side-chain for each residue. You select the interaction combinations you want to include. Options are: \"bb-bb\", \"sc-sc\", \"bb-sc\", \"sc-bb\". Where bb = backbone and sc = sidechain.\n",
    "\n",
    "4. **filter_by_avg_strength(average_strength_cut_off)** - PyContact calculates a per frame contact score/strength for each interaction. You can filter features by the average score. Values below the cut-off are removed. \n",
    "\n",
    "5. **filter_by_occupancy_by_class(min_occupancy)** - Special alternative to the the standard filter features by occupancy method. %occupancy is determined for each class (as opposed to whole dataset), meaning only observations from 1 class have to meet the cut-off to keep the feature. Only avaible to datasets with a categorical target variable (classification). \n",
    "\n",
    "\n",
    "Finally if at any point in time you want to reset any filtering you've already performed, you can use the following method: \n",
    "\n",
    "6. **reset_filtering()** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of filtering the dataset using the 4 available methods. \n",
    "supervised_dataset.reset_filtering() \n",
    "print(f\"Number of features before any filtering: {len(supervised_dataset.df_processed.columns)}\")\n",
    "\n",
    "# Features with a %occupancy of less than 25% are removed. \n",
    "supervised_dataset.filter_by_occupancy(min_occupancy=25)\n",
    "print(f\"Number of features after filtering by occupancy: {len(supervised_dataset.df_filtered.columns)}\")\n",
    "\n",
    "# No filtering performed here as all possible combinations are included. \n",
    "supervised_dataset.filter_by_interaction_type(\n",
    "    interaction_types_included=[\"Hbond\", ]) # \"Saltbr\", \"Hydrophobic\", \"Other\" # TODO\n",
    "print(f\"Number of features after NOT filtering by interaction type: {len(supervised_dataset.df_filtered.columns)}\")\n",
    "\n",
    "# No filtering performed here as all possible combinations are included. \n",
    "supervised_dataset.filter_by_main_or_side_chain(\n",
    "    main_side_chain_types_included=[\"bb-bb\", ]  # \"sc-sc\", \"bb-sc\", \"sc-bb\" # TODO \n",
    ")\n",
    "print(f\"Number of features after NOT filtering by main or side chain: {len(supervised_dataset.df_filtered.columns)}\")\n",
    "\n",
    "# Features with an average interaction strength less than 1.0 will be removed.  \n",
    "supervised_dataset.filter_by_avg_strength(\n",
    "    average_strength_cut_off=1.0,  \n",
    ")\n",
    "print(f\"Number of features after filtering by average interaction scores: {len(supervised_dataset.df_filtered.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at the class attributes of our SupervisedFeatureData() instance (we called it: supervised_dataset) using the special \"\\_\\_dict__\" method we can see two dataframes we could use in the machine learning to follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_dataset.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are: \n",
    "- 'df_processed' - The unfiltered dataframe, 3057 features\n",
    "- 'df_filtered' - The filtered dataframe. Less than 3057 features. \n",
    "\n",
    "In the following section we will use the filtered dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Perform the Machine Learning with the model_building.py module. \n",
    "\n",
    "Now we will setup and run the supervised machine learning (ML) on the retro aldolase enzyme. Here we will apply to ML to distinguish between catalytically active and inactive conformations of the enzyme towards catalysis of XXXX. \n",
    "\n",
    "Describe the ML in more detail TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_dataset.df_filtered[\"Target\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model.\n",
    "regress_model = model_building.RegressionModel(\n",
    "    dataset=supervised_dataset.df_filtered,\n",
    "    evaluation_split_ratio=0.15,\n",
    "    models_to_use=[\"XGBoost\", \"CatBoost\", \"Random_Forest\"],\n",
    "    scaling_method=\"min_max\",\n",
    "    out_dir=\"outputs/retro_aldol_ml_regression\",\n",
    "    cross_validation_splits=5,\n",
    "    cross_validation_repeats=3, \n",
    "    search_approach=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can go ahead and build the models.\n",
    "We have one optional parameter in the command below which is to save the models generated. This can be useful if you ever want to back and do the post-processing (described in steps 4 and 5) in the future for instance. \n",
    "\n",
    "If you set this to true all the files required will be saved to a folder called \"temporary_files\" in your current working directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_model.build_models(save_models=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the models now built, we can see the models seem to be xxxxx\n",
    "\n",
    "We can now evaluate the quality of the models on the validation dataset. Again, thankfully the accuracy is quite high.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regress_model.evaluate_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Work up the Machine Learning with the post_proccessing.py module. \n",
    "\n",
    "With this module, we can analyse our results in more detail to understand what features each model determined where important for distinguishing between each state. \n",
    "\n",
    "In order to perform the analysis we will need to load in the models previously generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we will make an instance of the SupervisedPostProcessor class.\n",
    "post_proc = post_proccessing.SupervisedPostProcessor(\n",
    "    out_dir=\"outputs/retro_aldol_ml_regression\",\n",
    ")\n",
    "\n",
    "# Option 1 - Load models from the instance of the RegressionModel class. \n",
    "post_proc.load_models_from_instance(supervised_model=regress_model)\n",
    "\n",
    "# Option 2 - Load models from disk.\n",
    "# post_proc.load_models_from_disk(models_to_use=[\"XGBoost\", \"CatBoost\", \"Random_Forest\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After preparing the class we can now determine the feature importances for each model.\n",
    "post_proc.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also project these per feature importances onto the per-residue level. \n",
    "# This is done by summing each residues features importances and normalising so that the residue\n",
    "#  with the greatest overall  \n",
    "post_proc.get_per_res_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, if we take a look at the class attributes we can see the per feature and \n",
    "# per residue importances were not just saved to disk, but are also now stored in the class\n",
    "# meaning you can analyse them here if you wish. \n",
    "print(post_proc.__dict__.keys())\n",
    "all_per_res_scores = post_proc.all_per_residue_scores\n",
    "all_feature_scores = post_proc.all_feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 Projecting the Results onto Protein Structures with the pymol_projections.py module. \n",
    " \n",
    "Naturally, we may want to visualise some of the results we have generated above onto a protein structure. We can take advantage of the functions provided in the pymol_projections.py module to do this. \n",
    "\n",
    "As the name suggests this will output [PyMOL](https://pymol.org/) compatible python scripts which can be run to represent the results at either the: \n",
    "\n",
    "1. Per feature level. (Cylinders are drawn between both residues in each feature, with the cylinder radii marking how large the relative importance is. \n",
    "2. Per residue level. The carbon alpha of each residue will be depicted as a sphere, with the sphere radii depicting the relative importance of the residue for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymol_projections.project_multiple_per_res_scores(\n",
    "    all_per_res_scores=all_per_res_scores,\n",
    "    out_dir=\"outputs/retro_aldol_ml_regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pymol_projections.project_multiple_per_feature_scores(\n",
    "    all_feature_scores=all_feature_scores,\n",
    "    numb_features=\"all\",\n",
    "    out_dir=\"outputs/retro_aldol_ml_regression\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ADD Picture of the outputs here as an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "add0b5d4ce7b8e8a859fa0dda4e7913231effb3978a57212389923662b8875fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('ML_Py3_8': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
