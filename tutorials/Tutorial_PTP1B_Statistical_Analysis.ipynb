{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial for Statistical Analysis on Simulations of PTP1B. \n",
    "\n",
    "In this jupyter notebook we will use the stat_modelling.py module to identify differences in the molecular interactions across PTP1B\n",
    "when the WPD-loop of PTP1B is in the Closed state, versus when the WPD-loop is in the Open state.\n",
    "This notebook will also cover all the pre- and post-processing steps requireds to prepare, analyse and visualise the results.\n",
    "\n",
    "The dataset used here is for WT PTP1B and is the same data as what was used in the manuscript. \n",
    "\n",
    "<center><img src=\"miscellaneous/PTP1B_Stat_Analysis_Banner.png\" alt=\"Drawing\" style=\"width: 70%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # note temporary... \n",
    "sys.path.append(\"..\") # note temporary...\n",
    "\n",
    "from key_interactions_finder import pycontact_processing\n",
    "from key_interactions_finder import data_preperation\n",
    "from key_interactions_finder import stat_modelling\n",
    "from key_interactions_finder import post_proccessing\n",
    "from key_interactions_finder import pymol_projections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Process PyContact files with the pycontact_processing.py module \n",
    "\n",
    "In this section we will work with the PyContact output files generated. \n",
    "Here we will merge our seperate runs together and remove any false interactions that can be generated by the PyContact library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycontact_files_horizontal = [\"PyContact_Per_Frame_Interactions_Block1.csv\", \"PyContact_Per_Frame_Interactions_Block2.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block3.csv\", \"PyContact_Per_Frame_Interactions_Block4.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block5.csv\", \"PyContact_Per_Frame_Interactions_Block6.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block7.csv\", \"PyContact_Per_Frame_Interactions_Block8.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block9.csv\", \"PyContact_Per_Frame_Interactions_Block10.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block11.csv\", \"PyContact_Per_Frame_Interactions_Block12.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block13.csv\", \"PyContact_Per_Frame_Interactions_Block14.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block15.csv\", \"PyContact_Per_Frame_Interactions_Block16.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block17.csv\", \"PyContact_Per_Frame_Interactions_Block18.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block19.csv\", \"PyContact_Per_Frame_Interactions_Block20.csv\",\n",
    "                              ]\n",
    "\n",
    "pycontact_dataset = pycontact_processing.PyContactInitializer(\n",
    "    pycontact_files=pycontact_files_horizontal,\n",
    "    multiple_files=True,\n",
    "    merge_files_method=\"horizontal\",  \n",
    "    remove_false_interactions=True,\n",
    "    in_dir=\"datasets/PTP1B_Data/example_horizontal_data/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As outputted above, we can inspect the newly prepared dataset by accessing the '.prepared_df' class attribute as follows:\n",
    "pycontact_dataset.prepared_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Prepare the Dataset for Statistical Analysis with the data_preperation.py module. \n",
    "\n",
    "In this step, we take our dataframe and merge our per frame classifications file to it.\n",
    "\n",
    "We can also optionally perform several forms of filtering on the PyContact features to select what types of interactions we would like to study.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we generate an instance of the SupervisedFeatureData class (because we have per frame target labels).\n",
    "classifications_file = \"datasets/PTP1B_Data/example_horizontal_data/WT_PTP1B_Class_Assingments.txt\"\n",
    "\n",
    "supervised_dataset = data_preperation.SupervisedFeatureData(\n",
    "    input_df=pycontact_dataset.prepared_df,\n",
    "    target_file=classifications_file,\n",
    "    is_classification=True,\n",
    "    header_present=False # If your target_file has a header present, set to True.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As stated above to access the newly generated dataframe we can use the class attribute as follows\n",
    "supervised_dataset.df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Optional Feature Filtering\n",
    "\n",
    "In the above dataframe we have 2791 columns (so 2790 features + 1 target). We can take all of these forward for the stastical analysis or we can perform some filtering in advance (the choice is yours). \n",
    "There are five built in filtering methods available to you to perform filtering:\n",
    "\n",
    "1. **filter_by_occupancy(min_occupancy)** - Remove features that have an %occupancy less than the provided cut-off. %Occupancy is the % of frames with a non 0 value, i.e. the interaction is present in that frame.\n",
    "\n",
    "2. **filter_by_interaction_type(interaction_types_included)** - PyContact defines four types of interactions (\"Hbond\", \"Saltbr\", \"Hydrophobic\", \"Other\"). You select the interactions your want to include.\n",
    "\n",
    "3. **filter_by_main_or_side_chain(main_side_chain_types_included)** - PyContact can also define if each interaction is primarily from the backbone or side-chain for each residue. You select the interaction combinations you want to include. Options are: \"bb-bb\", \"sc-sc\", \"bb-sc\", \"sc-bb\". Where bb = backbone and sc = sidechain.\n",
    "\n",
    "4. **filter_by_avg_strength(average_strength_cut_off)** - PyContact calculates a per frame contact score/strength for each interaction. You can filter features by the average score. Values below the cut-off are removed. \n",
    "\n",
    "5. **filter_by_occupancy_by_class(min_occupancy)** - Special alternative to the the standard filter features by occupancy method. %occupancy is determined for each class (as opposed to whole dataset), meaning only observations from 1 class have to meet the cut-off to keep the feature. Only avaible to datasets with a categorical target variable (classification). \n",
    "\n",
    "\n",
    "Finally if at any point in time you want to reset any filtering you've already performed, you can use the following method: \n",
    "\n",
    "6. **reset_filtering()** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of filtering the dataset using the 4 available methods. \n",
    "supervised_dataset.reset_filtering() \n",
    "print(f\"Number of features before any filtering: {len(supervised_dataset.df_processed.columns)}\")\n",
    "\n",
    "# Features with a %occupancy of less than 25% are removed. \n",
    "supervised_dataset.filter_by_occupancy_by_class(min_occupancy=25)\n",
    "print(f\"Number of features after filtering by occupancy: {len(supervised_dataset.df_filtered.columns)}\")\n",
    "\n",
    "# Remove features with interaction type \"Other\".  \n",
    "supervised_dataset.filter_by_interaction_type(\n",
    "    interaction_types_included=[\"Hbond\", \"Saltbr\", \"Hydrophobic\"]) \n",
    "print(f\"Number of features after filtering by interaction type: {len(supervised_dataset.df_filtered.columns)}\")\n",
    "\n",
    "# No filtering performed here as all possible combinations are included. \n",
    "supervised_dataset.filter_by_main_or_side_chain(\n",
    "    main_side_chain_types_included=[\"bb-bb\", \"sc-sc\", \"bb-sc\", \"sc-bb\"]  \n",
    ")\n",
    "print(f\"Number of features after NOT filtering by main or side chain: {len(supervised_dataset.df_filtered.columns)}\")\n",
    "\n",
    "# Features with an average interaction strength less than 0.5 will be removed. \n",
    "supervised_dataset.filter_by_avg_strength(\n",
    "    average_strength_cut_off=0.5,  \n",
    ")\n",
    "print(f\"Number of features after filtering by average interaction scores: {len(supervised_dataset.df_filtered.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we look at the class attributes of our SupervisedFeatureData() instance (we called it: supervised_dataset) using the special \"\\_\\_dict__\" method we can see two dataframes we could use in the stastical analysis to follow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supervised_dataset.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are: \n",
    "- 'df_processed' - The unfiltered dataframe, 2791 features\n",
    "- 'df_filtered' - The filtered dataframe. Less than 2791 features. \n",
    "\n",
    "In the following section we will use the filtered dataframe but either dataframe could be justified based on your goals. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Perform the Statistical Analysis with the stat_modelling.py module. \n",
    "\n",
    "Now we will perform the actual statistical modelling to compare the differences for each feature when the WPD-loop is in the closed and open state. \n",
    "\n",
    "This module only works with two classes (i.e. binary classification). You can select what classes you want to use in the next code block. \n",
    "\n",
    "With this module, we can calculate two different metrics to evaluate how different/similar each feature is when the protein is in the closed WPD-loop conformation or the open-WPD-loop conformation: \n",
    "\n",
    "1. The Jensen-Shannon distance. This xxxx. After first generating probability distributions of the interaction strengths for each feature when in the two states provided, this is calculated using the [implementation available in SciPy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html).\n",
    "2. Mutual information: This calculates the xxxx. This is calculated using the [implementation available in Scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html).\n",
    "\n",
    "In both cases, the higher the score, the more \"different\" the feature is when in the two different states.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_model = stat_modelling.ProteinStatModel(\n",
    "    dataset=supervised_dataset.df_filtered, \n",
    "    class_names=[\"Closed\", \"Open\"], # select the two class labels to compare. Has to be 2 labels. \n",
    "    out_dir=\"outputs/PTP1B_stat_analysis\",\n",
    "    interaction_types_included=[\"Hbond\", \"Saltbr\", \"Hydrophobic\", \"Other\"] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we can calculate the Jensen-Shannon distances (Took me about 20 seconds).\n",
    "stat_model.calc_js_distances()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can calculate the mutual informations (Took me about a minute).\n",
    "stat_model.calc_mutual_info_to_target()\n",
    "# TODO - IF I AM SAVING DATA HERE, I NEED IT TO PRINT THAT OUT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As printed above we can access the results from these calculations from the class instance's (we called it stat_model) attributes. \n",
    "mi_results = stat_model.mutual_infos\n",
    "js_results = stat_model.js_distances\n",
    "# stat_model.__dict__.keys() # uncomment to see all attributes available. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Work up the Statistical Analysis with the post_proccessing.py module. \n",
    "\n",
    "In this module we have access to 3 methods to analyse the results in more detail.\n",
    "\n",
    "1. We can use generate these results to be at the per-residue level, by summing (and then normalising) the feature scores that each residue is involved in.\n",
    "This can allow us to identify residues which seem to have the largest overall difference in interactions between each state. \n",
    "\n",
    "2. We can also try to predict the \"direction\" each feature favours. \n",
    "\n",
    "3. We can obtain the probability distrubitions of the interaction scores for a selected set of features to visually compare the distributions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First generate an instance of the class. \n",
    "post_proc = post_proccessing.StatisticalPostProcessor(\n",
    "    stat_model=stat_model,\n",
    "    out_dir=\"outputs/PTP1B_stat_analysis\"\n",
    ")\n",
    "\n",
    "# As you have already seen in the prior steps, we can take a look at class attributes as follows.\n",
    "# Note that some of these attributes will be empty until we run the next few code blocks.\n",
    "post_proc.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can run the get_per_res_importance() method, changing the stat_method accordingly.\n",
    "js_per_res_importances = post_proc.get_per_res_importance(\n",
    "    stat_method=\"jensen_shannon\")\n",
    "mi_per_res_importances = post_proc.get_per_res_importance(\n",
    "    stat_method=\"mutual_information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_proc.estimate_feature_directions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can plot the probability distrubitions for the most different Features. \n",
    "x_values, selected_prob_distribs = post_proc.get_probability_distributions(\n",
    "    number_features=10)\n",
    "\n",
    "# TODO - DESCRIBE THE OUTPUT, MAKE A NICE PLOTLY GRAPH :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5 Projecting the Results onto Protein Structures with the pymol_projections.py module. \n",
    " \n",
    "Naturally, we may want to visualise some of the results we have generated above onto a protein structure. We can take advantage of\n",
    "the functions provided in the pymol_projections.py module to do this. \n",
    "\n",
    "As the name suggests this will output [PyMOL](https://pymol.org/) compatible python scripts which can be run to represent the results\n",
    "at the: \n",
    "\n",
    "1. Per feature level. (Cylinders are drawn between each feature, with the cylinder radii marking how strong the relative difference is. \n",
    "2. Per residue level. The Carbon alpha of each residue will be depicted as a sphere, with the sphere radii depicting how strong the the relative difference is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write PyMOL compatable scripts for the per feature results.\n",
    "# Simply swap between the two statistical methods as shown below. \n",
    "pymol_projections.project_pymol_top_features(\n",
    "    per_feature_scores=stat_model.js_distances,\n",
    "    model_name=\"jenson_shannon\",\n",
    "    numb_features=150,\n",
    "    out_dir=\"outputs/PTP1B_stat_analysis\"\n",
    ")\n",
    "\n",
    "pymol_projections.project_pymol_top_features(\n",
    "    per_feature_scores=stat_model.mutual_infos,\n",
    "    model_name=\"mutual_information\",\n",
    "    numb_features=150,\n",
    "    out_dir=\"outputs/PTP1B_stat_analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write PyMOL compatable scripts for the per residue results.\n",
    "# Simply swap between the two statistical methods as shown below. \n",
    "pymol_projections.project_pymol_per_res_scores(\n",
    "    per_res_scores=js_per_res_importances,\n",
    "    model_name=\"jenson_shannon\",\n",
    "    out_dir=\"outputs/PTP1B_stat_analysis\"\n",
    ")\n",
    "\n",
    "pymol_projections.project_pymol_per_res_scores(\n",
    "    per_res_scores=mi_per_res_importances,\n",
    "    model_name=\"mutual_information\",\n",
    "    out_dir=\"outputs/PTP1B_stat_analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO ADD Picture of the outputs here as an example. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO Add below section as example way to calculate distance to target and make nice plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from MDAnalysis.analysis import distances\n",
    "import MDAnalysis as mda\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_file = \"datasets/PTP1B_data/WT_PTP1B_Phospho_Enzyme_Closed.pdb\"\n",
    "universe = mda.Universe(pdb_file)\n",
    "\n",
    "catres = \"resid 180 and name CA\"\n",
    "all_residues = \"resid 0-298 and name CA\"\n",
    "\n",
    "group1 = universe.select_atoms(catres)\n",
    "group2 = universe.select_atoms(all_residues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_arr = distances.distance_array(group1.positions, group2.positions, box=universe.dimensions)\n",
    "\n",
    "out_file = r\"C:\\Users\\Rory Crean\\Dropbox (lkgroup)\\Backup_HardDrive\\Postdoc\\MachLearnConformationalFeatures\\Workup\\PTP1B_Stats\\2_Non_PyMOL_Stats\\Distance_toCalpha_D181_Closed.txt\"\n",
    "np.savetxt(out_file, dist_arr.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "add0b5d4ce7b8e8a859fa0dda4e7913231effb3978a57212389923662b8875fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('ML_Py3_8': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
