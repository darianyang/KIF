{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial for Network Analysis on Simulations of PTP1B. \n",
    "\n",
    "In this jupyter notebook we will use the network_analysis.py module on the PTP1B PyContact dataset to generate inputs for various network analysis methods. \n",
    "\n",
    "There already exist several tools to perform network analysis and therefore..... \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This notebook will also cover all the pre-processing steps required alongside an example of how to use the program \n",
    "\n",
    "The dataset used here is for PTP1B is the same as what we used in the manuscript. \n",
    "\n",
    "This approach does not require class labels (i.e. unsupervised datasets are fine). \n",
    "\n",
    "If you do have class labels and want to study the differences between classes (for example to study alterations in allosteric networks in different conformational states), then you can first split your trajectory frames and perform the analysis seperartely. \n",
    "\n",
    "Here however, we shall combine the snapshots of PTP1B in both conformational states, (1) the closed WPD-loop state and (2) the open WPD-loop conformation. \n",
    "\n",
    "\n",
    "<center><img src=\"miscellaneous/PTP1B_Stat_Analysis_Banner.png\" alt=\"Drawing\" style=\"width: 70%\" /></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys # note temporary... \n",
    "sys.path.append(\"..\") # note temporary... \n",
    "\n",
    "from key_interactions_finder import pycontact_processing\n",
    "from key_interactions_finder import data_preperation\n",
    "from key_interactions_finder import network_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Process PyContact files with the pycontact_processing.py module \n",
    "\n",
    "In this section we will work with the PyContact output files generated. \n",
    "Here we will merge our seperate runs together and remove any false interactions that can be generated by the PyContact library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycontact_files_horizontal = [\"PyContact_Per_Frame_Interactions_Block1.csv\", \"PyContact_Per_Frame_Interactions_Block2.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block3.csv\", \"PyContact_Per_Frame_Interactions_Block4.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block5.csv\", \"PyContact_Per_Frame_Interactions_Block6.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block7.csv\", \"PyContact_Per_Frame_Interactions_Block8.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block9.csv\", \"PyContact_Per_Frame_Interactions_Block10.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block11.csv\", \"PyContact_Per_Frame_Interactions_Block12.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block13.csv\", \"PyContact_Per_Frame_Interactions_Block14.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block15.csv\", \"PyContact_Per_Frame_Interactions_Block16.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block17.csv\", \"PyContact_Per_Frame_Interactions_Block18.csv\",\n",
    "                              \"PyContact_Per_Frame_Interactions_Block19.csv\", \"PyContact_Per_Frame_Interactions_Block20.csv\",\n",
    "                              ]\n",
    "\n",
    "pycontact_dataset = pycontact_processing.PyContactInitializer(\n",
    "    pycontact_files=pycontact_files_horizontal,\n",
    "    multiple_files=True,\n",
    "    merge_files_method=\"horizontal\",  \n",
    "    remove_false_interactions=True,\n",
    "    in_dir=\"datasets/PTP1B_Data/example_horizontal_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As outputted above, we can inspect the newly prepared dataset by accessing the '.prepared_df' class attribute as follows:\n",
    "pycontact_dataset.prepared_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 (Optional) Prepare the Dataset for Network Analysis with the data_preperation.py module. \n",
    "\n",
    "In this step, we we will generate a instance of the UnsupervisedFeatureData class to prepare the dataset for the network analysis. We will use this instance to perform some (optional) filtering to limit the features we make use of in our analysis. \n",
    "\n",
    "If you want to skip this step, then you can simply take the dataframe produced in the prior step (pycontact_dataset.prepared_df) and use that in Step 3. I would recommend though to filter features with a low %occupancy as we will do in this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we generate an instance of the UnsupervisedFeatureData class (because we have per frame class labels).\n",
    "unsupervised_dataset = data_preperation.UnsupervisedFeatureData(\n",
    "    input_df=pycontact_dataset.prepared_df,\n",
    ")\n",
    "\n",
    "# See attributes availale to the class\n",
    "unsupervised_dataset.__dict__.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, our instance of the UnsupervisedFeatureData class has two attributes, the 'input_df' and the 'df_filtered' (currently empty). Once we use a filtering method in the next code block we will create a populated 'df_filtered' and be able to use it in the next step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this case, I am going to remove features/contacts that are: \n",
    "# 1. Not present for at least 50% of the simulation time.\n",
    "# 2. Of type \"Other\" - In PyContact lingo this means a van der Waals interaction between two residues\n",
    "# that are not both hydrophobic. \n",
    "unsupervised_dataset.filter_by_occupancy(min_occupancy=50)\n",
    "unsupervised_dataset.filter_by_interaction_type(\n",
    "    interaction_types_included=[\"Hbond\", \"Saltbr\", \"Hydrophobic\"])\n",
    "\n",
    "print(f\"Number of features before filtering: {len(unsupervised_dataset.input_df.columns)}\")\n",
    "print(f\"Number of features after filtering: {len(unsupervised_dataset.df_filtered.columns)}\")\n",
    "unsupervised_dataset.df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Perform the Network Analysis with the network_analysis.py module. \n",
    "\n",
    "Described\n",
    "\n",
    "\n",
    "Some Nomenclature:\n",
    "Node - DEFINE\n",
    "Edge - DEFINE\n",
    "\n",
    "Relating the above to biomolecules, we can normally think of a residue as a node in a network and a\n",
    "connection between two residues as an edge. \n",
    "\n",
    "Check out this paper if you're new to the topic --- LINK TODO. \n",
    "\n",
    "\n",
    "In Network/Correlation analysis in biomolecular simulations tend to require two sets of data:\n",
    "1. A correlation matrix for every node in the network to every other node.\n",
    "2. A distance matrix (sometimes called a distance/contact map) which says whether two nodes are in contact with each other (usually defined by whether they are in close contact with one another). \n",
    "\n",
    "In this section, we will generate both of these and save them so that you can make use of the plethora of methods out there as you wish. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First lets generate an instance of the CorrelationNetwork class. \n",
    "correlation_network = network_analysis.CorrelationNetwork(\n",
    "    dataset=unsupervised_dataset.df_filtered, \n",
    "    out_dir=\"outputs/PTP1B_network_analysis\"\n",
    ")\n",
    "\n",
    "# As before lets take a look at the class attributes\n",
    "correlation_network.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can see from the above, we have now generated a correlation matrix to each feature. \n",
    "correlation_network.feature_corr_matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst the above correlation matrix could be used for network analysis, it may be more intuituative to represent the data at the per residue level.\n",
    "With the below code block we can do exactly that. \n",
    "Note that this will both write the file to disk and return the object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_res_corr_matrix = correlation_network.gen_res_correl_matrix(\n",
    "    out_file=\"outputs/PTP1B_network_analysis/per_res_matrix.csv\"\n",
    ")\n",
    "per_res_corr_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alongside a correlation matrix we will also want to generate some form of contact map to define which nodes (residues) are in close proximity to one another. \n",
    "\n",
    "Here two main options are provided:\n",
    "\n",
    "1. Using the method \"gen_res_contact_matrix\". Here, we define residues as in contact with one another if they share an interaction in the dataframe. (Essentially using the column names to identify pairs of residues in contact).  \n",
    "2. Using the method \"heavy_atom_contact_map_from_pdb\". Here, we calculate the minimum heavy atom distance between each residue pair and if the minimun distance is below the defined distance cut-off (d_cut), the two atoms are considered in contact. \n",
    "    \n",
    "    * Note that if you have multiple PDB files, you can use the method \"heavy_atom_contact_map_from_multiple_pdbs\" instead. Here, if in any of the frames provided, the two residues are below the contact distance cut-off (d_cut), they will be considered in contact. \n",
    "\n",
    "Option 2 is the more standard method to determine close contacts and is what we will use in this tutorial as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Option 1:\n",
    "per_res_contact_matrix = correlation_network.gen_res_contact_matrix(\n",
    "    out_file=\"outputs/PTP1B_network_analysis/per_res_contact_matrix.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2, single PDB file example.  \n",
    "# heavy_atom_contact_map = network_analysis.heavy_atom_contact_map_from_pdb(\n",
    "#     pdb_file=\"datasets/PTP1B_data/WT_PTP1B_Phospho_Enzyme_Closed.pdb\",\n",
    "#     first_res=1,\n",
    "#     last_res=298,\n",
    "#     d_cut=5,\n",
    "#     out_file=\"outputs/PTP1B_network_analysis/heavy_atom_contact_map.csv\"\n",
    "# )\n",
    "\n",
    "# Option 2, multiple PDB files  \n",
    "network_analysis.heavy_atom_contact_map_from_multiple_pdbs(\n",
    "    pdb_files=[\"datasets/PTP1B_data/WT_PTP1B_Phospho_Enzyme_Closed.pdb\",\n",
    "               \"datasets/PTP1B_data/WT_PTP1B_Phospho_Enzyme_Open.pdb\"],\n",
    "    first_res=1,\n",
    "    last_res=298,\n",
    "    d_cut=5,\n",
    "    out_file=\"outputs/PTP1B_network_analysis/heavy_atom_contact_map_MultiPDB.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Run the Network Analysis of your Choice. \n",
    "\n",
    "There exists so many types of the network analysis and several packages developed towards this. For this reason I choose not to build this into this program. Instead, I provide an example of how to use Bio3D to analyse the results. There are many other methods/programs out there that require the same inputs as what we have produced here, so feel free to use what you wish of course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "add0b5d4ce7b8e8a859fa0dda4e7913231effb3978a57212389923662b8875fe"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('ML_Py3_8': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
